{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c0cd2e6-3647-48ea-90e6-be4b6520ae95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\banda\\\\Documents\\\\linux\\\\neuralnets\\\\Natural Language  Processing with Classification and Vector Spaces\\\\week2'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d1d8ab46-b099-4bc2-956a-f56aa0f1d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tweet\n",
    "import pdb\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "843d1b60-4ffd-416f-85ce-149412b640ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4a72c5d-e95c-41a3-8e1b-246ce6da8ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, ..., 0, 0, 0], dtype=int8),\n",
       " array([1, 1, 1, ..., 0, 0, 0], dtype=int8))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos), dtype=np.int8), np.zeros(len(train_neg), dtype=np.int8))\n",
    "test_y = np.append(np.ones(len(test_pos), dtype=np.int8), np.zeros(len(test_neg), dtype=np.int8))\n",
    "train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4980f40-0ae6-4f65-803b-485fa3cf8e2f",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f70a21e6-aa09-4a6b-9a74-5c0c44981770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['stuck', 'centr', 'right', 'clown', 'right', 'joker', 'left', '...', ':)'],\n",
       " \"@HumayAG 'Stuck in the centre right with you. Clowns to the right, jokers to the left...' :) @orgasticpotency @ahmedshaheed @AhmedSaeedGahaa\")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(test_pos[3]), test_pos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c58a7856-6c55-4c1d-a05e-7418a8e84378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            key = (word, y)\n",
    "            if key in result:\n",
    "                result[key] += 1\n",
    "            else:\n",
    "                result[key] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "# count_tweets({}, train_pos[:10], [1]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "99f977db-2d52-422a-9b82-ae032121e5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "labels = [1, 0, 0, 0, 0]\n",
    "count_tweets({}, tweets, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117965b2-d0f0-4306-ad02-d9e1c6960e3f",
   "metadata": {},
   "source": [
    "## Trianing Using Naive Bayes\n",
    "### How to train Naive Bayes Classifier\n",
    "* first identify number of classes(categories/labels) that you have.\n",
    "* probability for each class. calculate probability that the Document(tweet) is positive, and negative.\n",
    "  $$ P\\left( {D_{\\text{pos}}} \\right) = \\frac{D_{\\text{pos}}}{D} $$\n",
    "  $$ P\\left( {D_{\\text{neg}}} \\right) = \\frac{D_{\\text{pos}}}{D} $$\n",
    "\n",
    "### Prior and Logprior:\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative. In other words, if we have to pick a tweet out of all the tweet population, what is probability that it will be positive versus that it will be negative.\n",
    "\n",
    "So, prior = $ \\frac{P(D_{\\text{pos}})}{P(D_{\\text{pos}})} = \\frac{D_{\\text{pos}}}{D_{\\text{neg}}} $ \n",
    "\n",
    "logprior = $ \\log\\left( {P(D_{\\text{pos}})} \\right) - \\log\\left( P(D_{\\text{pos}}) \\right) = \\log\\left( D_{\\text{pos}} \\right) - \\log\\left( D_{\\text{neg}} \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61fe3fb-b2f0-432f-b19c-91c25ab6a05c",
   "metadata": {},
   "source": [
    "### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "* $ freq_\\text{pos} $ and $ freq_\\text{neg} $  are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "* $ N_\\text{pos} $ and $ N_\\text{pos} $ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "* $ V $ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    " $$ P(W_\\text{pos}) = \\frac{freq_\\text{pos} + 1}{N_\\text{pos} + V}$$\n",
    " $$ P(W_\\text{neg}) = \\frac{freq_\\text{neg} + 1}{N_\\text{neg} + V}$$\n",
    "+1 is added for smoothing(Laplace Smoothing).\n",
    "##### Why Add-One Smoothing?\n",
    "Without smoothing, if a word has  $ freq_\\text{pos} = 0$ in positive documents, the probability $ P(W_\\text{pos}) $ becomes 0. Since Naive Bayes multiplies probabilities across words, a zero probability for any word would make the entire document's probability for that class zero, even if other words strongly support the class.\n",
    "\n",
    "Adding $ +1 $ensures every word has at least a small non-zero probability.\n",
    "##### What does V do?\n",
    "$ V $ (vocabulary size) is added to the denominator to account for the fact that we added $ +1 $ to the numerator for every word. It adjusts the probabilities to ensure they remain valid (sum to 1 across the entire vocabulary). Without $ V $, the probabilities could become inflated.\n",
    "\n",
    "## Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "$$ loglikelihood = \\log\\left( {\\frac{P(W_\\text{pos})}{P(W_\\text{neg})}}  \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93b625d5-dabc-4d47-98c3-7c2ea9647e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)\n",
    "# freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240205e-a1c4-455d-838c-587801630b58",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. calculate V (number of unique words appeared in the freqs dictionary)\n",
    "2. calculate $ freq_\\text{pos} $ and $ freq_\\text{neg} $.\n",
    "3. calculate $ N_\\text{pos} $ and $ N_\\text{neg} $\n",
    "4. calculate $ D $, $ D_\\text{pos} $ and $ D_\\text{neg} $\n",
    "5. calculate logprior\n",
    "6. calculate log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "12b5bdf9-645a-4dc7-bab3-2477bfd90456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    vocab = set([item[0] for item in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    # Npos, Nneg\n",
    "    N_pos = N_neg = 0\n",
    "    for key, count in freqs.items():\n",
    "        if key[1] == 1:\n",
    "            N_pos += count\n",
    "        else:\n",
    "            N_neg += 1\n",
    "\n",
    "    # number of Documents(tweets)\n",
    "    D = len(train_x)\n",
    "    # number of positive Docs\n",
    "    D_pos = len(train_pos)\n",
    "    # number of negative Docs\n",
    "    D_neg = len(train_neg)\n",
    "\n",
    "    # logprior\n",
    "    logprior = np.log(D_pos/D_neg)\n",
    "    # i = 0\n",
    "    for word in vocab:\n",
    "        pos_freq = freqs.get((word, 1), 0)\n",
    "        neg_freq = freqs.get((word, 0), 0)\n",
    "        p_w_pos = (pos_freq + 1) / (N_pos + V)\n",
    "        p_w_neg = (neg_freq + 1) / (N_neg + V)\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "        # i+= 1\n",
    "\n",
    "    return logprior, loglikelihood\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b6da12a0-f630-4867-8b8c-051aca94ad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 18)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[('happi', 1)], freqs[('happi', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2fd7c9eb-a80f-4f81-a1f3-ef86e0312343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[('sad', 1)], freqs[('sad', 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35b129c7-439c-44b4-8fb3-ccb3978dced9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9085, np.float64(0.0))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprior, loglikelihood  = train_naive_bayes(freqs, train_x, train_y)\n",
    "len(loglikelihood), logprior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce75a60-55e4-4564-a023-d86911abb82c",
   "metadata": {},
   "source": [
    "## Part 3: Test your naive bayes\n",
    "Now that we have the logprior and loglikelihood, we can test the naive bayes function by making predicting on some tweets!\n",
    "\n",
    "**Implement** naive_bayes_predict\n",
    "**Instructions:** Implement the naive_bayes_predict function to make predictions on tweets.\n",
    "* The function takes in the tweet, logprior, loglikelihood.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "$$ p = logprior + \\sum_{i}^{N} \\left( loglikelihood_i \\right)$$\n",
    "\n",
    "##### Note\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets). This means that the ratio of positive to negative 1, and the logprior is 0.\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood. However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c89179e7-d669-413f-adc8-198b60bc8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    '''\n",
    "    words = process_tweet(tweet)\n",
    "    p = 0\n",
    "    p += logprior\n",
    "    for word in words:\n",
    "        p += loglikelihood.get(word, 0)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e60a132-8b91-4116-97ea-f5bb67ecc04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected outpt is: 0.6742938061462189\n",
      "The expected outpt is: 1.2488352442983173\n",
      "The expected outpt is: -0.8943221117676263\n",
      "The expected outpt is: -3.717683159380831\n",
      "The expected outpt is: -2.7401488022659573\n",
      "The expected outpt is: -2.7401488022659573\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = ['She smiled.', 'i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "for tweet in sample_tweets:\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f\"The expected outpt is: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "09ccacf1-bc8c-4193-a3e0-89294388720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: The corresponding labels for the list of tweets\n",
    "        logprior: The logprior\n",
    "        loglikelihood: A dictionary with the loglikelihoods for each word\n",
    "\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # This will store the final accuracy\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    y_hats = []  # This will store the predicted labels\n",
    "\n",
    "    for tweet, true_label in zip(test_x, test_y):\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    correct_predictions = sum([1 for y_hat, y in zip(y_hats, test_y) if y_hat == y])\n",
    "    error = (len(test_y) - correct_predictions) / len(test_y)\n",
    "    print(f\"{error=}\")\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66478da0-414c-4d67-80c5-e72cae2e65ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error=0.1195\n",
      "Naive Bayes accuracy = 0.8805\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
